{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter8.ipynb","provenance":[{"file_id":"https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb","timestamp":1630575336666}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ON6jU-cos5E1"},"source":["# 8章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"r6r9ATFJImOU"},"source":["# 8-1\n","!mkdir chap8\n","%cd ./chap8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hJ-pXOwXBzH"},"source":["# 8-2\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWsBlMRNhnnx"},"source":["# 8-3\n","import itertools\n","import random\n","import json\n","from tqdm import tqdm\n","import numpy as np\n","import unicodedata\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForTokenClassification\n","import pytorch_lightning as pl\n","\n","# 日本語学習済みモデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ilBB1q6zPvww"},"source":["# 8-4\n","normalize = lambda s: unicodedata.normalize(\"NFKC\",s)\n","print(f'ＡＢＣ -> {normalize(\"ＡＢＣ\")}' )  # 全角アルファベット\n","print(f'ABC -> {normalize(\"ABC\")}' )        # 半角アルファベット\n","print(f'１２３ -> {normalize(\"１２３\")}' )  # 全角数字\n","print(f'123 -> {normalize(\"123\")}' )        # 半角数字\n","print(f'アイウ -> {normalize(\"アイウ\")}' )  # 全角カタカナ\n","print(f'ｱｲｳ -> {normalize(\"ｱｲｳ\")}' )        # 半角カタカナ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OnK2OiMKhmt0"},"source":["# 8-5\n","class NER_tokenizer(BertJapaneseTokenizer):\n","       \n","    def encode_plus_tagged(self, text, entities, max_length):\n","        \"\"\"\n","        文章とそれに含まれる固有表現が与えられた時に、\n","        符号化とラベル列の作成を行う。\n","        \"\"\"\n","        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n","        entities = sorted(entities, key=lambda x: x['span'][0])\n","        splitted = [] # 分割後の文字列を追加していく\n","        position = 0\n","        for entity in entities:\n","            start = entity['span'][0]\n","            end = entity['span'][1]\n","            label = entity['type_id']\n","            # 固有表現ではないものには0のラベルを付与\n","            splitted.append({'text':text[position:start], 'label':0}) \n","            # 固有表現には、固有表現のタイプに対応するIDをラベルとして付与\n","            splitted.append({'text':text[start:end], 'label':label}) \n","            position = end\n","        splitted.append({'text': text[position:], 'label':0})\n","        splitted = [ s for s in splitted if s['text'] ] # 長さ0の文字列は除く\n","\n","        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける。\n","        tokens = [] # トークンを追加していく\n","        labels = [] # トークンのラベルを追加していく\n","        for text_splitted in splitted:\n","            text = text_splitted['text']\n","            label = text_splitted['label']\n","            tokens_splitted = self.tokenize(text)\n","            labels_splitted = [label] * len(tokens_splitted)\n","            tokens.extend(tokens_splitted)\n","            labels.extend(labels_splitted)\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids, \n","            max_length=max_length, \n","            padding='max_length', \n","            truncation=True\n","        ) # input_idsをencodingに変換\n","        # 特殊トークン[CLS]、[SEP]のラベルを0にする。\n","        labels = [0] + labels[:max_length-2] + [0] \n","        # 特殊トークン[PAD]のラベルを0にする。\n","        labels = labels + [0]*( max_length - len(labels) ) \n","        encoding['labels'] = labels\n","\n","        return encoding\n","\n","    def encode_plus_untagged(\n","        self, text, max_length=None, return_tensors=None\n","    ):\n","        \"\"\"\n","        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n","        \"\"\"\n","        # 文章のトークン化を行い、\n","        # それぞれのトークンと文章中の文字列を対応づける。\n","        tokens = [] # トークンを追加していく。\n","        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n","        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n","        for word in words:\n","            # 単語をサブワードに分割\n","            tokens_word = self.subword_tokenizer.tokenize(word) \n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == '[UNK]': # 未知語への対応\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                    token.replace('##','') for token in tokens_word\n","                ])\n","\n","        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n","        position = 0\n","        spans = [] # トークンの位置を追加していく。\n","        for token in tokens_original:\n","            l = len(token)\n","            while 1:\n","                if token != text[position:position+l]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+l])\n","                    position += l\n","                    break\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens) \n","        encoding = self.prepare_for_model(\n","            input_ids, \n","            max_length=max_length, \n","            padding='max_length' if max_length else False, \n","            truncation=True if max_length else False\n","        )\n","        sequence_length = len(encoding['input_ids'])\n","        # 特殊トークン[CLS]に対するダミーのspanを追加。\n","        spans = [[-1, -1]] + spans[:sequence_length-2] \n","        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n","        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n","\n","        # 必要に応じてtorch.Tensorにする。\n","        if return_tensors == 'pt':\n","            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n","\n","        return encoding, spans\n","\n","    def convert_bert_output_to_entities(self, text, labels, spans):\n","        \"\"\"\n","        文章、ラベル列の予測値、各トークンの位置から固有表現を得る。\n","        \"\"\"\n","        # labels, spansから特殊トークンに対応する部分を取り除く\n","        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n","        spans = [span for span in spans if span[0] != -1]\n","\n","        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n","        entities = []\n","        for label, group \\\n","            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n","            \n","            group = list(group)\n","            start = spans[group[0][0]][0]\n","            end = spans[group[-1][0]][1]\n","\n","            if label != 0: # ラベルが0以外ならば、新たな固有表現として追加。\n","                entity = {\n","                    \"name\": text[start:end],\n","                    \"span\": [start, end],\n","                    \"type_id\": label\n","                }\n","                entities.append(entity)\n","\n","        return entities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qh07iizk9Onf"},"source":["# 8-6\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5704VNOVt8Yg"},"source":["# 8-7\n","text = '昨日のみらい事務所との打ち合わせは順調だった。'\n","entities = [\n","    {'name': 'みらい事務所', 'span': [3,9], 'type_id': 1}\n","]\n","\n","encoding = tokenizer.encode_plus_tagged(\n","    text, entities, max_length=20\n",")\n","print(encoding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDik0FRIL38J"},"source":["# 8-8\n","text = '騰訊の英語名はTencent Holdings Ltdである。'\n","encoding, spans = tokenizer.encode_plus_untagged(\n","    text, return_tensors='pt'\n",")\n","print('# encoding')\n","print(encoding)\n","print('# spans')\n","print(spans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YHS30W_woM-E"},"source":["# 8-9\n","labels_predicted = [0,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0]\n","entities = tokenizer.convert_bert_output_to_entities(\n","    text, labels_predicted, spans\n",")\n","print(entities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jo45M23hSL_-"},"source":["# 8-10\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n","bert_tc = BertForTokenClassification.from_pretrained(\n","    MODEL_NAME, num_labels=4\n",")\n","bert_tc = bert_tc.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t344Ak2_0C2c"},"source":["# 8-11\n","text = 'AさんはB大学に入学した。'\n","\n","# 符号化を行い、各トークンの文章中での位置も特定しておく。\n","encoding, spans = tokenizer.encode_plus_untagged(\n","    text, return_tensors='pt'\n",") \n","encoding = { k: v.cuda() for k, v in encoding.items() } \n","\n","# BERTでトークン毎の分類スコアを出力し、スコアの最も高いラベルを予測値とする。\n","with torch.no_grad():\n","    output = bert_tc(**encoding)\n","    scores = output.logits\n","    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n","\n","# ラベル列を固有表現に変換\n","entities = tokenizer.convert_bert_output_to_entities(\n","    text, labels_predicted, spans\n",")\n","print(entities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QmFZVpb208CK"},"source":["# 8-12\n","data = [\n","    {\n","        'text': 'AさんはB大学に入学した。',\n","        'entities': [\n","            {'name': 'A', 'span': [0, 1], 'type_id': 2},\n","            {'name': 'B大学', 'span': [4, 7], 'type_id': 1}\n","        ]\n","    },\n","    {\n","        'text': 'CDE株式会社は新製品「E」を販売する。',\n","        'entities': [\n","            {'name': 'CDE株式会社', 'span': [0, 7], 'type_id': 1},\n","            {'name': 'E', 'span': [12, 13], 'type_id': 3}\n","        ]\n","    }\n","]\n","\n","# 各データを符号化し、データローダを作成する。\n","max_length=32\n","dataset_for_loader = []\n","for sample in data:\n","    text = sample['text']\n","    entities = sample['entities']\n","    encoding = tokenizer.encode_plus_tagged(\n","        text, entities, max_length=max_length\n","    )\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset_for_loader.append(encoding)\n","dataloader = DataLoader(dataset_for_loader, batch_size=len(data))\n","\n","# ミニバッチを取り出し損失を得る。\n","for batch in dataloader:\n","    batch = { k: v.cuda() for k, v in batch.items() } # GPU\n","    output = bert_tc(**batch) # BERTへ入力\n","    loss = output.loss # 損失"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-4aEmfGf9qT"},"source":["# 8-13\n","!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HpwR_RXZlX80"},"source":["# 8-14\n","# データのロード\n","dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))\n","\n","# 固有表現のタイプとIDを対応付る辞書 \n","type_id_dict = {\n","    \"人名\": 1,\n","    \"法人名\": 2,\n","    \"政治的組織名\": 3,\n","    \"その他の組織名\": 4,\n","    \"地名\": 5,\n","    \"施設名\": 6,\n","    \"製品名\": 7,\n","    \"イベント名\": 8\n","}\n","\n","# カテゴリーをラベルに変更、文字列の正規化する。\n","for sample in dataset:\n","    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n","    for e in sample[\"entities\"]:\n","        e['type_id'] = type_id_dict[e['type']]\n","        del e['type']\n","\n","# データセットの分割\n","random.shuffle(dataset)\n","n = len(dataset)\n","n_train = int(n*0.6)\n","n_val = int(n*0.2)\n","dataset_train = dataset[:n_train]\n","dataset_val = dataset[n_train:n_train+n_val]\n","dataset_test = dataset[n_train+n_val:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDGDvIPIlX-s"},"source":["# 8-15\n","def create_dataset(tokenizer, dataset, max_length):\n","    \"\"\"\n","    データセットをデータローダに入力できる形に整形。\n","    \"\"\"\n","    dataset_for_loader = []\n","    for sample in dataset:\n","        text = sample['text']\n","        entities = sample['entities']\n","        encoding = tokenizer.encode_plus_tagged(\n","            text, entities, max_length=max_length\n","        )\n","        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","        dataset_for_loader.append(encoding)\n","    return dataset_for_loader\n","\n","# トークナイザのロード\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n","\n","# データセットの作成\n","max_length = 128\n","dataset_train_for_loader = create_dataset(\n","    tokenizer, dataset_train, max_length\n",")\n","dataset_val_for_loader = create_dataset(\n","    tokenizer, dataset_val, max_length\n",")\n","\n","# データローダの作成\n","dataloader_train = DataLoader(\n","    dataset_train_for_loader, batch_size=32, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"446u47Os_Arh"},"source":["# 8-16\n","# PyTorch Lightningのモデル\n","class BertForTokenClassification_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, num_labels, lr):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.bert_tc = BertForTokenClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","        \n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_tc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","        \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_tc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","        \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n","\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/'\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1,\n","    max_epochs=5,\n","    callbacks=[checkpoint]\n",")\n","\n","# ファインチューニング\n","model = BertForTokenClassification_pl(\n","    MODEL_NAME, num_labels=9, lr=1e-5\n",")\n","trainer.fit(model, dataloader_train, dataloader_val)\n","best_model_path = checkpoint.best_model_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9OU3oPuxCEWz"},"source":["# 8-17\n","def predict(text, tokenizer, bert_tc):\n","    \"\"\"\n","    BERTで固有表現抽出を行うための関数。\n","    \"\"\"\n","    # 符号化\n","    encoding, spans = tokenizer.encode_plus_untagged(\n","        text, return_tensors='pt'\n","    )\n","    encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","    # ラベルの予測値の計算\n","    with torch.no_grad():\n","        output = bert_tc(**encoding)\n","        scores = output.logits\n","        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist() \n","\n","    # ラベル列を固有表現に変換\n","    entities = tokenizer.convert_bert_output_to_entities(\n","        text, labels_predicted, spans\n","    )\n","\n","    return entities\n","\n","# トークナイザのロード\n","tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)\n","\n","# ファインチューニングしたモデルをロードし、GPUにのせる。\n","model = BertForTokenClassification_pl.load_from_checkpoint(\n","    best_model_path\n",")\n","bert_tc = model.bert_tc.cuda()\n","\n","# 固有表現抽出\n","# 注：以下ではコードのわかりやすさのために、1データづつ処理しているが、\n","# バッチ化して処理を行った方が処理時間は短い\n","entities_list = [] # 正解の固有表現を追加していく。\n","entities_predicted_list = [] # 抽出された固有表現を追加していく。\n","for sample in tqdm(dataset_test):\n","    text = sample['text']\n","    entities_predicted = predict(text, tokenizer, bert_tc) # BERTで予測\n","    entities_list.append(sample['entities'])\n","    entities_predicted_list.append( entities_predicted )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-Fu6xJZ29HJ"},"source":["# 8-18\n","print(\"# 正解\")\n","print(entities_list[0])\n","print(\"# 抽出\")\n","print(entities_predicted_list[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeZzmNigiDsD"},"source":["# 8-19\n","def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n","    \"\"\"\n","    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n","    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n","    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n","    \"\"\"\n","    num_entities = 0 # 固有表現(正解)の個数\n","    num_predictions = 0 # BERTにより予測された固有表現の個数\n","    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n","\n","    # それぞれの文章で予測と正解を比較。\n","    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n","    for entities, entities_predicted \\\n","        in zip(entities_list, entities_predicted_list):\n","\n","        if type_id:\n","            entities = [ e for e in entities if e['type_id'] == type_id ]\n","            entities_predicted = [ \n","                e for e in entities_predicted if e['type_id'] == type_id\n","            ]\n","            \n","        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n","        set_entities = set( get_span_type(e) for e in entities )\n","        set_entities_predicted = \\\n","            set( get_span_type(e) for e in entities_predicted )\n","\n","        num_entities += len(entities)\n","        num_predictions += len(entities_predicted)\n","        num_correct += len( set_entities & set_entities_predicted )\n","\n","    # 指標を計算\n","    precision = num_correct/num_predictions # 適合率\n","    recall = num_correct/num_entities # 再現率\n","    f_value = 2*precision*recall/(precision+recall) # F値\n","\n","    result = {\n","        'num_entities': num_entities,\n","        'num_predictions': num_predictions,\n","        'num_correct': num_correct,\n","        'precision': precision,\n","        'recall': recall,\n","        'f_value': f_value\n","    }\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVbxf1FRlYBU"},"source":["# 8-20\n","print( evaluate_model(entities_list, entities_predicted_list) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tD9sFRUu4Z4c"},"source":["# 8-21\n","class NER_tokenizer_BIO(BertJapaneseTokenizer):\n","\n","    # 初期化時に固有表現のカテゴリーの数`num_entity_type`を\n","    # 受け入れるようにする。\n","    def __init__(self, *args, **kwargs):\n","        self.num_entity_type = kwargs.pop('num_entity_type')\n","        super().__init__(*args, **kwargs)\n","\n","    def encode_plus_tagged(self, text, entities, max_length):\n","        \"\"\"\n","        文章とそれに含まれる固有表現が与えられた時に、\n","        符号化とラベル列の作成を行う。\n","        \"\"\"\n","        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n","        splitted = [] # 分割後の文字列を追加していく\n","        position = 0\n","        for entity in entities:\n","            start = entity['span'][0]\n","            end = entity['span'][1]\n","            label = entity['type_id']\n","            splitted.append({'text':text[position:start], 'label':0})\n","            splitted.append({'text':text[start:end], 'label':label})\n","            position = end\n","        splitted.append({'text': text[position:], 'label':0})\n","        splitted = [ s for s in splitted if s['text'] ]\n","\n","        # 分割されたそれぞれの文章をトークン化し、ラベルをつける。\n","        tokens = [] # トークンを追加していく\n","        labels = [] # ラベルを追加していく\n","        for s in splitted:\n","            tokens_splitted = self.tokenize(s['text'])\n","            label = s['label']\n","            if label > 0: # 固有表現\n","                # まずトークン全てにI-タグを付与\n","                labels_splitted =  \\\n","                    [ label + self.num_entity_type ] * len(tokens_splitted)\n","                # 先頭のトークンをB-タグにする\n","                labels_splitted[0] = label\n","            else: # それ以外\n","                labels_splitted =  [0] * len(tokens_splitted)\n","            \n","            tokens.extend(tokens_splitted)\n","            labels.extend(labels_splitted)\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids, \n","            max_length=max_length, \n","            padding='max_length',\n","            truncation=True\n","        ) \n","\n","        # ラベルに特殊トークンを追加\n","        labels = [0] + labels[:max_length-2] + [0]\n","        labels = labels + [0]*( max_length - len(labels) )\n","        encoding['labels'] = labels\n","\n","        return encoding\n","\n","    def encode_plus_untagged(\n","        self, text, max_length=None, return_tensors=None\n","    ):\n","        \"\"\"\n","        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n","        IO法のトークナイザのencode_plus_untaggedと同じ\n","        \"\"\"\n","        # 文章のトークン化を行い、\n","        # それぞれのトークンと文章中の文字列を対応づける。\n","        tokens = [] # トークンを追加していく。\n","        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n","        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n","        for word in words:\n","            # 単語をサブワードに分割\n","            tokens_word = self.subword_tokenizer.tokenize(word) \n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == '[UNK]': # 未知語への対応\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                    token.replace('##','') for token in tokens_word\n","                ])\n","\n","        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n","        position = 0\n","        spans = [] # トークンの位置を追加していく。\n","        for token in tokens_original:\n","            l = len(token)\n","            while 1:\n","                if token != text[position:position+l]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+l])\n","                    position += l\n","                    break\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens) \n","        encoding = self.prepare_for_model(\n","            input_ids, \n","            max_length=max_length, \n","            padding='max_length' if max_length else False, \n","            truncation=True if max_length else False\n","        )\n","        sequence_length = len(encoding['input_ids'])\n","        # 特殊トークン[CLS]に対するダミーのspanを追加。\n","        spans = [[-1, -1]] + spans[:sequence_length-2] \n","        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n","        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n","\n","        # 必要に応じてtorch.Tensorにする。\n","        if return_tensors == 'pt':\n","            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n","\n","        return encoding, spans\n","\n","    @staticmethod\n","    def Viterbi(scores_bert, num_entity_type, penalty=10000):\n","        \"\"\"\n","        Viterbiアルゴリズムで最適解を求める。\n","        \"\"\"\n","        m = 2*num_entity_type + 1\n","        penalty_matrix = np.zeros([m, m])\n","        for i in range(m):\n","            for j in range(1+num_entity_type, m):\n","                if not ( (i == j) or (i+num_entity_type == j) ): \n","                    penalty_matrix[i,j] = penalty\n","        \n","        path = [ [i] for i in range(m) ]\n","        scores_path = scores_bert[0] - penalty_matrix[0,:]\n","        scores_bert = scores_bert[1:]\n","\n","        for scores in scores_bert:\n","            assert len(scores) == 2*num_entity_type + 1\n","            score_matrix = np.array(scores_path).reshape(-1,1) \\\n","                + np.array(scores).reshape(1,-1) \\\n","                - penalty_matrix\n","            scores_path = score_matrix.max(axis=0)\n","            argmax = score_matrix.argmax(axis=0)\n","            path_new = []\n","            for i, idx in enumerate(argmax):\n","                path_new.append( path[idx] + [i] )\n","            path = path_new\n","\n","        labels_optimal = path[np.argmax(scores_path)]\n","        return labels_optimal\n","\n","    def convert_bert_output_to_entities(self, text, scores, spans):\n","        \"\"\"\n","        文章、分類スコア、各トークンの位置から固有表現を得る。\n","        分類スコアはサイズが（系列長、ラベル数）の2次元配列\n","        \"\"\"\n","        assert len(spans) == len(scores)\n","        num_entity_type = self.num_entity_type\n","        \n","        # 特殊トークンに対応する部分を取り除く\n","        scores = [score for score, span in zip(scores, spans) if span[0]!=-1]\n","        spans = [span for span in spans if span[0]!=-1]\n","\n","        # Viterbiアルゴリズムでラベルの予測値を決める。\n","        labels = self.Viterbi(scores, num_entity_type)\n","\n","        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n","        entities = []\n","        for label, group \\\n","            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n","            \n","            group = list(group)\n","            start = spans[group[0][0]][0]\n","            end = spans[group[-1][0]][1]\n","\n","            if label != 0: # 固有表現であれば\n","                if 1 <= label <= num_entity_type:\n","                     # ラベルが`B-`ならば、新しいentityを追加\n","                    entity = {\n","                        \"name\": text[start:end],\n","                        \"span\": [start, end],\n","                        \"type_id\": label\n","                    }\n","                    entities.append(entity)\n","                else:\n","                    # ラベルが`I-`ならば、直近のentityを更新\n","                    entity['span'][1] = end \n","                    entity['name'] = text[entity['span'][0]:entity['span'][1]]\n","                \n","        return entities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvyVJCgH4Z6M"},"source":["# 8-22\n","# トークナイザのロード\n","# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n","tokenizer = NER_tokenizer_BIO.from_pretrained(\n","    MODEL_NAME,\n","    num_entity_type=8 \n",")\n","\n","# データセットの作成\n","max_length = 128\n","dataset_train_for_loader = create_dataset(\n","    tokenizer, dataset_train, max_length\n",")\n","dataset_val_for_loader = create_dataset(\n","    tokenizer, dataset_val, max_length\n",")\n","\n","# データローダの作成\n","dataloader_train = DataLoader(\n","    dataset_train_for_loader, batch_size=32, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ykfw0rCA4Z9N"},"source":["# 8-23\n","\n","# ファインチューニング\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model_BIO/'\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1,\n","    max_epochs=5,\n","    callbacks=[checkpoint]\n",")\n","\n","# PyTorch Lightningのモデルのロード\n","num_entity_type = 8\n","num_labels = 2*num_entity_type+1\n","model = BertForTokenClassification_pl(\n","    MODEL_NAME, num_labels=num_labels, lr=1e-5\n",")\n","\n","# ファインチューニング\n","trainer.fit(model, dataloader_train, dataloader_val)\n","best_model_path = checkpoint.best_model_path\n","\n","# 性能評価\n","model = BertForTokenClassification_pl.load_from_checkpoint(\n","    best_model_path\n",") \n","bert_tc = model.bert_tc.cuda()\n","\n","entities_list = [] # 正解の固有表現を追加していく\n","entities_predicted_list = [] # 抽出された固有表現を追加していく\n","for sample in tqdm(dataset_test):\n","    text = sample['text']\n","    encoding, spans = tokenizer.encode_plus_untagged(\n","        text, return_tensors='pt'\n","    )\n","    encoding = { k: v.cuda() for k, v in encoding.items() } \n","    \n","    with torch.no_grad():\n","        output = bert_tc(**encoding)\n","        scores = output.logits\n","        scores = scores[0].cpu().numpy().tolist()\n","        \n","    # 分類スコアを固有表現に変換する\n","    entities_predicted = tokenizer.convert_bert_output_to_entities(\n","        text, scores, spans\n","    )\n","\n","    entities_list.append(sample['entities'])\n","    entities_predicted_list.append( entities_predicted )\n","\n","print(evaluate_model(entities_list, entities_predicted_list))"],"execution_count":null,"outputs":[]}]}