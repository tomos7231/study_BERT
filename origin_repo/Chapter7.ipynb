{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter7.ipynb","provenance":[{"file_id":"https://github.com/stockmarkteam/bert-book/blob/master/Chapter7.ipynb","timestamp":1630574288605}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CWPivw5Ss1Hk"},"source":["# 7章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"LvCX0ZnVJ1WD"},"source":["# 7-1\n","!mkdir chap7\n","%cd ./chap7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0iMot3XGIhtD"},"source":["# 7-2\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"87bW8wO5IhtF"},"source":["# 7-3\n","import random\n","import glob\n","import json\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertModel\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HFcRL7nnhbX"},"source":["# 7-4\n","class BertForSequenceClassificationMultiLabel(torch.nn.Module):\n","    \n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        # BertModelのロード\n","        self.bert = BertModel.from_pretrained(model_name) \n","        # 線形変換を初期化しておく\n","        self.linear = torch.nn.Linear(\n","            self.bert.config.hidden_size, num_labels\n","        ) \n","\n","    def forward(\n","        self, \n","        input_ids=None, \n","        attention_mask=None, \n","        token_type_ids=None, \n","        labels=None\n","    ):\n","        # データを入力しBERTの最終層の出力を得る。\n","        bert_output = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids)\n","        last_hidden_state = bert_output.last_hidden_state\n","        \n","        # [PAD]以外のトークンで隠れ状態の平均をとる\n","        averaged_hidden_state = \\\n","            (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n","            / attention_mask.sum(1, keepdim=True)\n","        \n","        # 線形変換\n","        scores = self.linear(averaged_hidden_state) \n","        \n","        # 出力の形式を整える。\n","        output = {'logits': scores}\n","\n","        # labelsが入力に含まれていたら、損失を計算し出力する。\n","        if labels is not None: \n","            loss = torch.nn.BCEWithLogitsLoss()(scores, labels.float())\n","            output['loss'] = loss\n","            \n","        # 属性でアクセスできるようにする。\n","        output = type('bert_output', (object,), output) \n","\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbWDC5z4x_kP"},"source":["# 7-5\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","bert_scml = BertForSequenceClassificationMultiLabel(\n","    MODEL_NAME, num_labels=2\n",") \n","bert_scml = bert_scml.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_ep4ddFjz-O"},"source":["# 7-6\n","text_list = [\n","    '今日の仕事はうまくいったが、体調があまり良くない。',\n","    '昨日は楽しかった。'\n","]\n","\n","labels_list = [\n","    [1, 1],\n","    [0, 1]\n","]\n","\n","# データの符号化\n","encoding = tokenizer(\n","    text_list, \n","    padding='longest',  \n","    return_tensors='pt'\n",")\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","labels = torch.tensor(labels_list).cuda()\n","\n","# BERTへデータを入力し分類スコアを得る。\n","with torch.no_grad():\n","    output = bert_scml(**encoding)\n","scores = output.logits\n","\n","# スコアが正ならば、そのカテゴリーを選択する。\n","labels_predicted = ( scores > 0 ).int()\n","\n","# 精度の計算\n","num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n","accuracy = num_correct/labels.size(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrXA5KgXmX-m"},"source":["# 7-7\n","# データの符号化\n","encoding = tokenizer(\n","    text_list, \n","    padding='longest',  \n","    return_tensors='pt'\n",")\n","encoding['labels'] = torch.tensor(labels_list) # 入力にlabelsを含める。\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","output = bert_scml(**encoding)\n","loss = output.loss # 損失"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HJ9Tbr6PIhtF"},"source":["# 7-8\n","# データのダウンロード\n","!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip\n","# データの解凍\n","!unzip chABSA-dataset.zip "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgXcOtz6fLge"},"source":["# 7-9\n","data = json.load(open('chABSA-dataset/e00030_ann.json'))\n","print( data['sentences'][0] )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l33ix4WDIhtG"},"source":["# 7-10\n","category_id = {'negative':0, 'neutral':1 , 'positive':2}\n","\n","dataset = []\n","for file in glob.glob('chABSA-dataset/*.json'):\n","    data = json.load(open(file))\n","    # 各データから文章（text）を抜き出し、ラベル（'labels'）を作成\n","    for sentence in data['sentences']:\n","        text = sentence['sentence'] \n","        labels = [0,0,0]\n","        for opinion in sentence['opinions']:\n","            labels[category_id[opinion['polarity']]] = 1\n","        sample = {'text': text, 'labels': labels}\n","        dataset.append(sample)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4Na8gOPHhya"},"source":["# 7-11\n","print(dataset[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"igPtmux1IhtI"},"source":["# 7-12\n","# トークナイザのロード\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 各データの形式を整える\n","max_length = 128\n","dataset_for_loader = []\n","for sample in dataset:\n","    text = sample['text']\n","    labels = sample['labels']\n","    encoding = tokenizer(\n","        text,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True\n","    )\n","    encoding['labels'] = labels\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset_for_loader.append(encoding)\n","\n","# データセットの分割\n","random.shuffle(dataset_for_loader) \n","n = len(dataset_for_loader)\n","n_train = int(0.6*n)\n","n_val = int(0.2*n)\n","dataset_train = dataset_for_loader[:n_train] # 学習データ\n","dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n","dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n","\n","#　データセットからデータローダを作成\n","dataloader_train = DataLoader(\n","    dataset_train, batch_size=32, shuffle=True\n",") \n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y3dO-kBIhtI"},"source":["# 7-13\n","class BertForSequenceClassificationMultiLabel_pl(pl.LightningModule):\n","\n","    def __init__(self, model_name, num_labels, lr):\n","        super().__init__()\n","        self.save_hyperparameters() \n","        self.bert_scml = BertForSequenceClassificationMultiLabel(\n","            model_name, num_labels=num_labels\n","        ) \n","\n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_scml(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","        \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_scml(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","\n","    def test_step(self, batch, batch_idx):\n","        labels = batch.pop('labels')\n","        output = self.bert_scml(**batch)\n","        scores = output.logits\n","        labels_predicted = ( scores > 0 ).int()\n","        num_correct = ( labels_predicted == labels ).all(-1).sum().item()\n","        accuracy = num_correct/scores.size(0)\n","        self.log('accuracy', accuracy)\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n","\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=5,\n","    callbacks = [checkpoint]\n",")\n","\n","model = BertForSequenceClassificationMultiLabel_pl(\n","    MODEL_NAME, \n","    num_labels=3, \n","    lr=1e-5\n",")\n","trainer.fit(model, dataloader_train, dataloader_val)\n","test = trainer.test(test_dataloaders=dataloader_test)\n","print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"My3WI8Qd7yVJ"},"source":["# 7-14\n","# 入力する文章\n","text_list = [\n","    \"今期は売り上げが順調に推移したが、株価は低迷の一途を辿っている。\",\n","    \"昨年から黒字が減少した。\",\n","    \"今日の飲み会は楽しかった。\"\n","]\n","\n","# モデルのロード\n","best_model_path = checkpoint.best_model_path\n","model = BertForSequenceClassificationMultiLabel_pl.load_from_checkpoint(best_model_path)\n","bert_scml = model.bert_scml.cuda()\n","\n","# データの符号化\n","encoding = tokenizer(\n","    text_list, \n","    padding = 'longest',\n","    return_tensors='pt'\n",")\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","# BERTへデータを入力し分類スコアを得る。\n","with torch.no_grad():\n","    output = bert_scml(**encoding)\n","scores = output.logits\n","labels_predicted = ( scores > 0 ).int().cpu().numpy().tolist()\n","\n","# 結果を表示\n","for text, label in zip(text_list, labels_predicted):\n","    print('--')\n","    print(f'入力：{text}')\n","    print(f'出力：{label}')"],"execution_count":null,"outputs":[]}]}