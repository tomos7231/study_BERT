{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter10.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Cao0hx_ts_xb"},"source":["# 10章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"BDX6Gi6xiCOY"},"source":["# 10-1\n","!mkdir chap10\n","%cd ./chap10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hJ-pXOwXBzH"},"source":["# 10-2\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_BGiKTflI39"},"source":["# 10-3\n","import random\n","import glob\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertModel\n","\n","# BERTの日本語モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r97ZbgVeZ-Hi"},"source":["# 10-4\n","#データのダウンロード\n","!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n","#ファイルの解凍\n","!tar -zxf ldcc-20140209.tar.gz "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9YGEfZUAxea"},"source":["# 10-5\n","# カテゴリーのリスト\n","category_list = [\n","    'dokujo-tsushin',\n","    'it-life-hack',\n","    'kaden-channel',\n","    'livedoor-homme',\n","    'movie-enter',\n","    'peachy',\n","    'smax',\n","    'sports-watch',\n","    'topic-news'\n","]\n","\n","# トークナイザとモデルのロード\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","model = BertModel.from_pretrained(MODEL_NAME)\n","model = model.cuda()\n","\n","# 各データの形式を整える\n","max_length = 256\n","sentence_vectors = [] # 文章ベクトルを追加していく。\n","labels = [] # ラベルを追加していく。\n","for label, category in enumerate(tqdm(category_list)):\n","    for file in glob.glob(f'./text/{category}/{category}*'):\n","        # 記事から文章を抜き出し、符号化を行う。\n","        lines = open(file).read().splitlines()\n","        text = '\\n'.join(lines[3:])\n","        encoding = tokenizer(\n","            text, \n","            max_length=max_length, \n","            padding='max_length', \n","            truncation=True, \n","            return_tensors='pt'\n","        )\n","        encoding = { k: v.cuda() for k, v in encoding.items() } \n","        attention_mask = encoding['attention_mask']\n","\n","        # 文章ベクトルを計算\n","        # BERTの最終層の出力を平均を計算する。（ただし、[PAD]は除く。）\n","        with torch.no_grad():\n","            output = model(**encoding)\n","            last_hidden_state = output.last_hidden_state \n","            averaged_hidden_state = \\\n","                (last_hidden_state*attention_mask.unsqueeze(-1)).sum(1) \\\n","                / attention_mask.sum(1, keepdim=True) \n","\n","        # 文章ベクトルとラベルを追加\n","        sentence_vectors.append(averaged_hidden_state[0].cpu().numpy())\n","        labels.append(label)\n","\n","# それぞれをnumpy.ndarrayにする。\n","sentence_vectors = np.vstack(sentence_vectors)\n","labels = np.array(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4h6wmubg5joK"},"source":["# 10-6\n","sentence_vectors_pca = PCA(n_components=2).fit_transform(sentence_vectors) \n","print(sentence_vectors_pca.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tupdek04-_0j"},"source":["# 10-7\n","plt.figure(figsize=(10,10))\n","for label in range(9):\n","    plt.subplot(3,3,label+1)\n","    index = labels == label\n","    plt.plot(\n","        sentence_vectors_pca[:,0], \n","        sentence_vectors_pca[:,1], \n","        'o', \n","        markersize=1, \n","        color=[0.7, 0.7, 0.7]\n","    )\n","    plt.plot(\n","        sentence_vectors_pca[index,0], \n","        sentence_vectors_pca[index,1], \n","        'o', \n","        markersize=2, \n","        color='k'\n","    )\n","    plt.title(category_list[label])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6PPni1WHqLK"},"source":["# 10-8\n","sentence_vectors_tsne = TSNE(n_components=2).fit_transform(sentence_vectors)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YIi2D2rhBknu"},"source":["# 10-9\n","plt.figure(figsize=(10,10))\n","for label in range(9):\n","    plt.subplot(3,3,label+1)\n","    index = labels == label\n","    plt.plot(\n","        sentence_vectors_tsne[:,0],\n","        sentence_vectors_tsne[:,1], \n","        'o', \n","        markersize=1, \n","        color=[0.7, 0.7, 0.7]\n","    )\n","    plt.plot(\n","        sentence_vectors_tsne[index,0],\n","        sentence_vectors_tsne[index,1], \n","        'o',\n","        markersize=2,\n","        color='k'\n","    )\n","    plt.title(category_list[label])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kp3U8tB-I46h"},"source":["# 10-10\n","# 先にノルムを1にしておく。\n","norm = np.linalg.norm(sentence_vectors, axis=1, keepdims=True) \n","sentence_vectors_normalized = sentence_vectors / norm\n","\n","# 類似度行列を計算する。\n","# 類似度行列の(i,j)要素はi番目の記事とj番目の記事の類似度を表している。\n","sim_matrix = sentence_vectors_normalized.dot(sentence_vectors_normalized.T)\n","\n","# 入力と同じ記事が出力されることを避けるため、\n","# 類似度行列の対角要素の値を小さくしておく。\n","np.fill_diagonal(sim_matrix, -1)\n","\n","# 類似度が高い記事のインデックスを得る\n","similar_news = sim_matrix.argmax(axis=1) \n","\n","# 類似文章検索により選ばれた記事とカテゴリーが同一であった記事の割合を計算\n","input_news_categories = labels\n","output_news_categories = labels[similar_news]\n","num_correct = ( input_news_categories == output_news_categories ).sum()\n","accuracy = num_correct / labels.shape[0]\n","\n","print(f\"Accuracy: {accuracy:.2f}\")"],"execution_count":null,"outputs":[]}]}