{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter9.ipynb","provenance":[{"file_id":"https://github.com/stockmarkteam/bert-book/blob/master/Chapter9.ipynb","timestamp":1630577154754}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OQ4zTAe6s78f"},"source":["# 9章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。\n","- 本章で用いる「[日本語Wikipedia入力誤りデータセット](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88)」は現在バージョン2が公開されていますが、本章では[バージョン1](https://nlp.ist.i.kyoto-u.ac.jp/?%E6%97%A5%E6%9C%AC%E8%AA%9EWikipedia%E5%85%A5%E5%8A%9B%E8%AA%A4%E3%82%8A%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88v1)を用いています。\n","\n"]},{"cell_type":"code","metadata":{"id":"YYAc8uDgGXNZ"},"source":["# 9-1\n","!mkdir chap9\n","%cd ./chap9"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13L45ApiGXNa"},"source":["# 9-2\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ULhAjFPGXNa"},"source":["# 9-3\n","import random\n","from tqdm import tqdm\n","import unicodedata\n","\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForMaskedLM\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習済みモデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MC1yL-SSGXNb"},"source":["# 9-4\n","class SC_tokenizer(BertJapaneseTokenizer):\n","       \n","    def encode_plus_tagged(\n","        self, wrong_text, correct_text, max_length=128\n","    ):\n","        \"\"\"\n","        ファインチューニング時に使用。\n","        誤変換を含む文章と正しい文章を入力とし、\n","        符号化を行いBERTに入力できる形式にする。\n","        \"\"\"\n","        # 誤変換した文章をトークン化し、符号化\n","        encoding = self(\n","            wrong_text, \n","            max_length=max_length, \n","            padding='max_length', \n","            truncation=True\n","        )\n","        # 正しい文章をトークン化し、符号化\n","        encoding_correct = self(\n","            correct_text,\n","            max_length=max_length,\n","            padding='max_length',\n","            truncation=True\n","        ) \n","        # 正しい文章の符号をラベルとする\n","        encoding['labels'] = encoding_correct['input_ids'] \n","\n","        return encoding\n","\n","    def encode_plus_untagged(\n","        self, text, max_length=None, return_tensors=None\n","    ):\n","        \"\"\"\n","        文章を符号化し、それぞれのトークンの文章中の位置も特定しておく。\n","        \"\"\"\n","        # 文章のトークン化を行い、\n","        # それぞれのトークンと文章中の文字列を対応づける。\n","        tokens = [] # トークンを追加していく。\n","        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n","        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n","        for word in words:\n","            # 単語をサブワードに分割\n","            tokens_word = self.subword_tokenizer.tokenize(word) \n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == '[UNK]': # 未知語への対応\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                    token.replace('##','') for token in tokens_word\n","                ])\n","\n","        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n","        position = 0\n","        spans = [] # トークンの位置を追加していく。\n","        for token in tokens_original:\n","            l = len(token)\n","            while 1:\n","                if token != text[position:position+l]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+l])\n","                    position += l\n","                    break\n","\n","        # 符号化を行いBERTに入力できる形式にする。\n","        input_ids = self.convert_tokens_to_ids(tokens) \n","        encoding = self.prepare_for_model(\n","            input_ids, \n","            max_length=max_length, \n","            padding='max_length' if max_length else False, \n","            truncation=True if max_length else False\n","        )\n","        sequence_length = len(encoding['input_ids'])\n","        # 特殊トークン[CLS]に対するダミーのspanを追加。\n","        spans = [[-1, -1]] + spans[:sequence_length-2] \n","        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n","        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n","\n","        # 必要に応じてtorch.Tensorにする。\n","        if return_tensors == 'pt':\n","            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n","\n","        return encoding, spans\n","\n","    def convert_bert_output_to_text(self, text, labels, spans):\n","        \"\"\"\n","        推論時に使用。\n","        文章と、各トークンのラベルの予測値、文章中での位置を入力とする。\n","        そこから、BERTによって予測された文章に変換。\n","        \"\"\"\n","        assert len(spans) == len(labels)\n","\n","        # labels, spansから特殊トークンに対応する部分を取り除く\n","        labels = [label for label, span in zip(labels, spans) if span[0]!=-1]\n","        spans = [span for span in spans if span[0]!=-1]\n","\n","        # BERTが予測した文章を作成\n","        predicted_text = ''\n","        position = 0\n","        for label, span in zip(labels, spans):\n","            start, end = span\n","            if position != start: # 空白の処理\n","                predicted_text += text[position:start]\n","            predicted_token = self.convert_ids_to_tokens(label)\n","            predicted_token = predicted_token.replace('##', '')\n","            predicted_token = unicodedata.normalize(\n","                'NFKC', predicted_token\n","            ) \n","            predicted_text += predicted_token\n","            position = end\n","        \n","        return predicted_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBTlINuoGXNc"},"source":["# 9-5\n","tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e27NIdKYGXNc"},"source":["# 9-6\n","wrong_text = '優勝トロフィーを変換した'\n","correct_text = '優勝トロフィーを返還した'\n","encoding = tokenizer.encode_plus_tagged(\n","    wrong_text, correct_text, max_length=12\n",")\n","print(encoding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLBWoRIcGXNd"},"source":["# 9-7\n","wrong_text = '優勝トロフィーを変換した'\n","encoding, spans = tokenizer.encode_plus_untagged(\n","    wrong_text, return_tensors='pt'\n",")\n","print('# encoding')\n","print(encoding)\n","print('# spans')\n","print(spans)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7gH4oB9GXNd"},"source":["# 9-8\n","predicted_labels = [2, 759, 18204, 11, 8274, 15, 10, 3]\n","predicted_text = tokenizer.convert_bert_output_to_text(\n","    wrong_text, predicted_labels, spans\n",")\n","print(predicted_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zCFGE4xbGXNe","jupyter":{"outputs_hidden":false}},"source":["# 9-9\n","bert_mlm = BertForMaskedLM.from_pretrained(MODEL_NAME)\n","bert_mlm = bert_mlm.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J2DR-ls8GXNf"},"source":["# 9-10\n","text = '優勝トロフィーを変換した。'\n","\n","# 符号化とともに各トークンの文章中の位置を計算しておく。\n","encoding, spans = tokenizer.encode_plus_untagged(\n","    text, return_tensors='pt'\n",")\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","# BERTに入力し、トークン毎にスコアの最も高いトークンのIDを予測値とする。\n","with torch.no_grad():\n","    output = bert_mlm(**encoding)\n","    scores = output.logits\n","    labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n","    \n","# ラベル列を文章に変換\n","predict_text = tokenizer.convert_bert_output_to_text(\n","    text, labels_predicted, spans\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wc0mIVlnGXNf"},"source":["# 9-11\n","data = [\n","    {\n","        'wrong_text': '優勝トロフィーを変換した。',\n","        'correct_text': '優勝トロフィーを返還した。',\n","    },\n","    {\n","        'wrong_text': '人と森は強制している。',\n","        'correct_text': '人と森は共生している。',\n","    }\n","]\n","\n","# 各データを符号化し、データローダへ入力できるようにする。\n","max_length=32\n","dataset_for_loader = []\n","for sample in data:\n","    wrong_text = sample['wrong_text']\n","    correct_text = sample['correct_text']\n","    encoding = tokenizer.encode_plus_tagged(\n","        wrong_text, correct_text, max_length=max_length\n","    )\n","    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","    dataset_for_loader.append(encoding)\n","\n","# データローダを作成\n","dataloader = DataLoader(dataset_for_loader, batch_size=2)\n","\n","# ミニバッチをBERTへ入力し、損失を計算。\n","for batch in dataloader:\n","    encoding = { k: v.cuda() for k, v in batch.items() }\n","    output = bert_mlm(**encoding)\n","    loss = output.loss # 損失"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0KBJLE4GXNg"},"source":["# 9-12\n","!curl -L \"https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd.tar.gz&name=JWTD.tar.gz\" -o JWTD.tar.gz\n","!tar zxvf JWTD.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-lflu5sGXNg","jupyter":{"outputs_hidden":false}},"source":["# 9-13\n","def create_dataset(data_df):\n","\n","    tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n","\n","    def check_token_count(row):\n","        \"\"\"\n","        誤変換の文章と正しい文章でトークンに対応がつくかどうかを判定。\n","        （条件は上の文章を参照）\n","        \"\"\"\n","        wrong_text_tokens = tokenizer.tokenize(row['wrong_text'])\n","        correct_text_tokens = tokenizer.tokenize(row['correct_text'])\n","        if len(wrong_text_tokens) != len(correct_text_tokens):\n","            return False\n","        \n","        diff_count = 0\n","        threthold_count = 2\n","        for wrong_text_token, correct_text_token \\\n","            in zip(wrong_text_tokens, correct_text_tokens):\n","\n","            if wrong_text_token != correct_text_token:\n","                diff_count += 1\n","                if diff_count > threthold_count:\n","                    return False\n","        return True\n","\n","    def normalize(text):\n","        \"\"\"\n","        文字列の正規化\n","        \"\"\"\n","        text = text.strip()\n","        text = unicodedata.normalize('NFKC', text)\n","        return text\n","\n","    # 漢字の誤変換のデータのみを抜き出す。\n","    category_type = 'kanji-conversion'\n","    data_df.query('category == @category_type', inplace=True) \n","    data_df.rename(\n","        columns={'pre_text': 'wrong_text', 'post_text': 'correct_text'}, \n","        inplace=True\n","    )\n","    \n","    # 誤変換と正しい文章をそれぞれ正規化し、\n","    # それらの間でトークン列に対応がつくもののみを抜き出す。\n","    data_df['wrong_text'] = data_df['wrong_text'].map(normalize) \n","    data_df['correct_text'] = data_df['correct_text'].map(normalize)\n","    kanji_conversion_num = len(data_df)\n","    data_df = data_df[data_df.apply(check_token_count, axis=1)]\n","    same_tokens_count_num = len(data_df)\n","    print(\n","        f'- 漢字誤変換の総数：{kanji_conversion_num}',\n","        f'- トークンの対応関係のつく文章の総数: {same_tokens_count_num}',\n","        f'  (全体の{same_tokens_count_num/kanji_conversion_num*100:.0f}%)',\n","        sep = '\\n'\n","    )\n","    return data_df[['wrong_text', 'correct_text']].to_dict(orient='records')\n","\n","# データのロード\n","train_df = pd.read_json(\n","    './jwtd/train.jsonl', orient='records', lines=True\n",")\n","test_df = pd.read_json(\n","    './jwtd/test.jsonl', orient='records', lines=True\n",")\n","\n","# 学習用と検証用データ\n","print('学習と検証用のデータセット：')\n","dataset = create_dataset(train_df)\n","random.shuffle(dataset)\n","n = len(dataset)\n","n_train = int(n*0.8)\n","dataset_train = dataset[:n_train]\n","dataset_val = dataset[n_train:]\n","\n","# テストデータ\n","print('テスト用のデータセット：')\n","dataset_test = create_dataset(test_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlr9SXDNGXNh"},"source":["# 9-14\n","def create_dataset_for_loader(tokenizer, dataset, max_length):\n","    \"\"\"\n","    データセットをデータローダに入力可能な形式にする。\n","    \"\"\"\n","    dataset_for_loader = []\n","    for sample in tqdm(dataset):\n","        wrong_text = sample['wrong_text']\n","        correct_text = sample['correct_text']\n","        encoding = tokenizer.encode_plus_tagged(\n","            wrong_text, correct_text, max_length=max_length\n","        )\n","        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","        dataset_for_loader.append(encoding)\n","    return dataset_for_loader\n","\n","tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n","\n","# データセットの作成\n","max_length = 32\n","dataset_train_for_loader = create_dataset_for_loader(\n","    tokenizer, dataset_train, max_length\n",")\n","dataset_val_for_loader = create_dataset_for_loader(\n","    tokenizer, dataset_val, max_length\n",")\n","\n","# データローダの作成\n","dataloader_train = DataLoader(\n","    dataset_train_for_loader, batch_size=32, shuffle=True\n",")\n","dataloader_val = DataLoader(dataset_val_for_loader, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-j7R6z0GXNh"},"source":["# 9-15\n","class BertForMaskedLM_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, lr):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.bert_mlm = BertForMaskedLM.from_pretrained(model_name)\n","        \n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_mlm(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss)\n","        return loss\n","        \n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_mlm(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss)\n","   \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n","\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/'\n",")\n","\n","trainer = pl.Trainer(\n","    gpus=1,\n","    max_epochs=5,\n","    callbacks=[checkpoint]\n",")\n","\n","# ファインチューニング\n","model = BertForMaskedLM_pl(MODEL_NAME, lr=1e-5)\n","trainer.fit(model, dataloader_train, dataloader_val)\n","best_model_path = checkpoint.best_model_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2eNTz6OsvVP"},"source":["# 9-16\n","def predict(text, tokenizer, bert_mlm):\n","    \"\"\"\n","    文章を入力として受け、BERTが予測した文章を出力\n","    \"\"\"\n","    # 符号化\n","    encoding, spans = tokenizer.encode_plus_untagged(\n","        text, return_tensors='pt'\n","    ) \n","    encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","    # ラベルの予測値の計算\n","    with torch.no_grad():\n","        output = bert_mlm(**encoding)\n","        scores = output.logits\n","        labels_predicted = scores[0].argmax(-1).cpu().numpy().tolist()\n","\n","    # ラベル列を文章に変換\n","    predict_text = tokenizer.convert_bert_output_to_text(\n","        text, labels_predicted, spans\n","    )\n","\n","    return predict_text\n","\n","# いくつかの例に対してBERTによる文章校正を行ってみる。\n","text_list = [\n","    'ユーザーの試行に合わせた楽曲を配信する。',\n","    'メールに明日の会議の史料を添付した。',\n","    '乳酸菌で牛乳を発行するとヨーグルトができる。',\n","    '突然、子供が帰省を発した。'\n","]\n","\n","# トークナイザ、ファインチューニング済みのモデルのロード\n","tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)\n","model = BertForMaskedLM_pl.load_from_checkpoint(best_model_path)\n","bert_mlm = model.bert_mlm.cuda()\n","\n","for text in text_list:\n","    predict_text = predict(text, tokenizer, bert_mlm) # BERTによる予測\n","    print('---')\n","    print(f'入力：{text}')\n","    print(f'出力：{predict_text}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zwqdG4SGXNi","jupyter":{"outputs_hidden":false}},"source":["# 9-17\n","# BERTで予測を行い、正解数を数える。\n","correct_num = 0 \n","for sample in tqdm(dataset_test):\n","    wrong_text = sample['wrong_text']\n","    correct_text = sample['correct_text']\n","    predict_text = predict(wrong_text, tokenizer, bert_mlm) # BERT予測\n","   \n","    if correct_text == predict_text: # 正解の場合\n","        correct_num += 1\n","\n","print(f'Accuracy: {correct_num/len(dataset_test):.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUj8v6dPGXNj"},"source":["# 9-18\n","correct_position_num = 0 # 正しく誤変換の漢字を特定できたデータの数\n","for sample in tqdm(dataset_test):\n","    wrong_text = sample['wrong_text']\n","    correct_text = sample['correct_text']\n","    \n","    # 符号化\n","    encoding = tokenizer(wrong_text)\n","    wrong_input_ids = encoding['input_ids'] # 誤変換の文の符合列\n","    encoding = {k: torch.tensor([v]).cuda() for k,v in encoding.items()}\n","    correct_encoding = tokenizer(correct_text)\n","    correct_input_ids = correct_encoding['input_ids'] # 正しい文の符合列\n","    \n","    # 文章を予測\n","    with torch.no_grad():\n","        output = bert_mlm(**encoding)\n","        scores = output.logits\n","        # 予測された文章のトークンのID\n","        predict_input_ids = scores[0].argmax(-1).cpu().numpy().tolist() \n","\n","    # 特殊トークンを取り除く\n","    wrong_input_ids = wrong_input_ids[1:-1]\n","    correct_input_ids =  correct_input_ids[1:-1]\n","    predict_input_ids =  predict_input_ids[1:-1]\n","    \n","    # 誤変換した漢字を特定できているかを判定\n","    # 符合列を比較する。\n","    detect_flag = True\n","    for wrong_token, correct_token, predict_token \\\n","        in zip(wrong_input_ids, correct_input_ids, predict_input_ids):\n","\n","        if wrong_token == correct_token: # 正しいトークン\n","            # 正しいトークンなのに誤って別のトークンに変換している場合\n","            if wrong_token != predict_token: \n","                detect_flag = False\n","                break\n","        else: # 誤変換のトークン\n","            # 誤変換のトークンなのに、そのままにしている場合\n","            if wrong_token == predict_token: \n","                detect_flag = False\n","                break\n","\n","    if detect_flag: # 誤変換の漢字の位置を正しく特定できた場合\n","        correct_position_num += 1\n","        \n","print(f'Accuracy: {correct_position_num/len(dataset_test):.2f}')"],"execution_count":null,"outputs":[]}]}