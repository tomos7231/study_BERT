{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Chapter6.ipynb","provenance":[{"file_id":"https://github.com/stockmarkteam/bert-book/blob/master/Chapter6.ipynb","timestamp":1630571793610}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DKcIdYD2sySs"},"source":["# 6章\n","- 以下で実行するコードには確率的な処理が含まれていることがあり、コードの出力結果と本書に記載されている出力例が異なることがあります。"]},{"cell_type":"code","metadata":{"id":"BDX6Gi6xiCOY"},"source":["# 6-1\n","!mkdir chap6\n","%cd ./chap6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hJ-pXOwXBzH"},"source":["# 6-2\n","!pip install transformers==4.5.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.2.10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_BGiKTflI39"},"source":["# 6-3\n","import random\n","import glob\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzgAG-1VpLd7"},"source":["# 6-4\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","bert_sc = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME, num_labels=2\n",")\n","bert_sc = bert_sc.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6EbYOsCGzaC"},"source":["# 6-5\n","text_list = [\n","    \"この映画は面白かった。\",\n","    \"この映画の最後にはがっかりさせられた。\",\n","    \"この映画を見て幸せな気持ちになった。\"\n","]\n","label_list = [1,0,1]\n","\n","# データの符号化\n","encoding = tokenizer(\n","    text_list, \n","    padding = 'longest',\n","    return_tensors='pt'\n",")\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","labels = torch.tensor(label_list).cuda()\n","\n","# 推論\n","with torch.no_grad():\n","    output = bert_sc.forward(**encoding)\n","scores = output.logits # 分類スコア\n","labels_predicted = scores.argmax(-1) # スコアが最も高いラベル\n","num_correct = (labels_predicted==labels).sum().item() # 正解数\n","accuracy = num_correct/labels.size(0) # 精度\n","\n","print(\"# scores:\")\n","print(scores.size())\n","print(\"# predicted labels:\")\n","print(labels_predicted)\n","print(\"# accuracy:\")\n","print(accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtKgd11pGyiE"},"source":["# 6-6\n","# 符号化\n","encoding = tokenizer(\n","    text_list, \n","    padding='longest',\n","    return_tensors='pt'\n",") \n","encoding['labels'] = torch.tensor(label_list) # 入力にラベルを加える。\n","encoding = { k: v.cuda() for k, v in encoding.items() }\n","\n","# ロスの計算\n","output = bert_sc(**encoding)\n","loss = output.loss # 損失の取得\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r97ZbgVeZ-Hi"},"source":["# 6-7\n","#データのダウンロード\n","!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n","#ファイルの解凍\n","!tar -zxf ldcc-20140209.tar.gz "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMUJ3rscgG2z"},"source":["# 6-8\n","!cat ./text/it-life-hack/it-life-hack-6342280.txt # ファイルを表示"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49pchD2z6JhM"},"source":["# 6-9\n","# データローダーの作成\n","dataset_for_loader = [\n","    {'data':torch.tensor([0,1]), 'labels':torch.tensor(0)},\n","    {'data':torch.tensor([2,3]), 'labels':torch.tensor(1)},\n","    {'data':torch.tensor([4,5]), 'labels':torch.tensor(2)},\n","    {'data':torch.tensor([6,7]), 'labels':torch.tensor(3)},\n","]\n","loader = DataLoader(dataset_for_loader, batch_size=2)\n","\n","# データセットからミニバッチを取り出す\n","for idx, batch in enumerate(loader):\n","    print(f'# batch {idx}')\n","    print(batch)\n","    ## ファインチューニングではここでミニバッチ毎の処理を行う"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_1f6IbMVbaH"},"source":["# 6-10\n","loader = DataLoader(dataset_for_loader, batch_size=2, shuffle=True)\n","\n","for idx, batch in enumerate(loader):\n","    print(f'# batch {idx}')\n","    print(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9YGEfZUAxea"},"source":["# 6-11\n","# カテゴリーのリスト\n","category_list = [\n","    'dokujo-tsushin',\n","    'it-life-hack',\n","    'kaden-channel',\n","    'livedoor-homme',\n","    'movie-enter',\n","    'peachy',\n","    'smax',\n","    'sports-watch',\n","    'topic-news'\n","]\n","\n","# トークナイザのロード\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 各データの形式を整える\n","max_length = 128\n","dataset_for_loader = []\n","for label, category in enumerate(tqdm(category_list)):\n","    for file in glob.glob(f'./text/{category}/{category}*'):\n","        lines = open(file).read().splitlines()\n","        text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す。\n","        encoding = tokenizer(\n","            text,\n","            max_length=max_length, \n","            padding='max_length',\n","            truncation=True\n","        )\n","        encoding['labels'] = label # ラベルを追加\n","        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","        dataset_for_loader.append(encoding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drP8IYLVBFh_"},"source":["# 6-12\n","print(dataset_for_loader[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHY9Os6NJlip"},"source":["# 6-13\n","# データセットの分割\n","random.shuffle(dataset_for_loader) # ランダムにシャッフル\n","n = len(dataset_for_loader)\n","n_train = int(0.6*n)\n","n_val = int(0.2*n)\n","dataset_train = dataset_for_loader[:n_train] # 学習データ\n","dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n","dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n","\n","# データセットからデータローダを作成\n","# 学習データはshuffle=Trueにする。\n","dataloader_train = DataLoader(\n","    dataset_train, batch_size=32, shuffle=True\n",") \n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffaUyGcoVj8l"},"source":["# 6-14\n","class BertForSequenceClassification_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, num_labels, lr):\n","        # model_name: Transformersのモデルの名前\n","        # num_labels: ラベルの数\n","        # lr: 学習率\n","\n","        super().__init__()\n","        \n","        # 引数のnum_labelsとlrを保存。\n","        # 例えば、self.hparams.lrでlrにアクセスできる。\n","        # チェックポイント作成時にも自動で保存される。\n","        self.save_hyperparameters() \n","\n","        # BERTのロード\n","        self.bert_sc = BertForSequenceClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","        \n","    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n","    # batch_idxはミニバッチの番号であるが今回は使わない。\n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n","        return loss\n","        \n","    # 検証データのミニバッチが与えられた時に、\n","    # 検証データを評価する指標を計算する関数を書く。\n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n","\n","    # テストデータのミニバッチが与えられた時に、\n","    # テストデータを評価する指標を計算する関数を書く。\n","    def test_step(self, batch, batch_idx):\n","        labels = batch.pop('labels') # バッチからラベルを取得\n","        output = self.bert_sc(**batch)\n","        labels_predicted = output.logits.argmax(-1)\n","        num_correct = ( labels_predicted == labels ).sum().item()\n","        accuracy = num_correct/labels.size(0) #精度\n","        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n","\n","    # 学習に用いるオプティマイザを返す関数を書く。\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyR6de1TqfW9"},"source":["# 6-15\n","# 学習時にモデルの重みを保存する条件を指定\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","# 学習の方法を指定\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=10,\n","    callbacks = [checkpoint]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgk48zEqIJKh"},"source":["# 6-16\n","# PyTorch Lightningモデルのロード\n","model = BertForSequenceClassification_pl(\n","    MODEL_NAME, num_labels=9, lr=1e-5\n",")\n","\n","# ファインチューニングを行う。\n","trainer.fit(model, dataloader_train, dataloader_val) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h68P7MG-JSh9"},"source":["# 6-17\n","best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n","print('ベストモデルのファイル: ', checkpoint.best_model_path)\n","print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-r9stqZqBdW"},"source":["# 6-18\n","%load_ext tensorboard\n","%tensorboard --logdir ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bx0L0Ehr1tM"},"source":["# 6-19\n","test = trainer.test(test_dataloaders=dataloader_test)\n","print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbJAUdrStSgI"},"source":["# 6-20\n","# PyTorch Lightningモデルのロード\n","model = BertForSequenceClassification_pl.load_from_checkpoint(\n","    best_model_path\n",") \n","\n","# Transformers対応のモデルを./model_transformesに保存\n","model.bert_sc.save_pretrained('./model_transformers') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcho1B0BtfV0"},"source":["# 6-21\n","bert_sc = BertForSequenceClassification.from_pretrained(\n","    './model_transformers'\n",")"],"execution_count":null,"outputs":[]}]}